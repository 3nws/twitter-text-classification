{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9432/1777459274.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mmodels_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../models/NN_model_seven_12008898266757515530_0.7899888157844543'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mtokenizers_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../tokenizers/12008898266757515530.pkl'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizers_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Enes\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Enes\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Enes\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\saving\\saved_model\\load.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(path, compile, options)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[0mnodes_to_load\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeras_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_node\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m   loaded = tf.__internal__.saved_model.load_partial(\n\u001b[1;32m--> 142\u001b[1;33m       path, nodes_to_load, options=options)\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m   \u001b[1;31m# Finalize the loaded layers and remove the extra tracked dependencies.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Enes\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36mload_partial\u001b[1;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[0;32m    840\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0mnode\u001b[0m \u001b[0mpaths\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfilter\u001b[0m \u001b[0mto\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mobjects\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    841\u001b[0m   \"\"\"\n\u001b[1;32m--> 842\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    843\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Enes\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36mload_internal\u001b[1;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[0;32m    973\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m         loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\n\u001b[1;32m--> 975\u001b[1;33m                             ckpt_options, options, filters)\n\u001b[0m\u001b[0;32m    976\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m         raise FileNotFoundError(\n",
      "\u001b[1;32mc:\\Users\\Enes\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[0mlibrary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmeta_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0msaved_object_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_proto\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m             wrapper_function=_WrapperFunction))\n\u001b[0m\u001b[0;32m    153\u001b[0m     \u001b[1;31m# Store a set of all concrete functions that have been set up with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;31m# captures.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Enes\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\saved_model\\function_deserialization.py\u001b[0m in \u001b[0;36mload_function_def_library\u001b[1;34m(library, saved_object_graph, load_shared_name_suffix, wrapper_function)\u001b[0m\n\u001b[0;32m    407\u001b[0m           \u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m           \u001b[0mstructured_input_signature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstructured_input_signature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m           structured_outputs=structured_outputs)\n\u001b[0m\u001b[0;32m    410\u001b[0m     \u001b[1;31m# Restores gradients for function-call ops (not the same as ops that use\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[1;31m# custom gradients)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Enes\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\function_def_to_graph.py\u001b[0m in \u001b[0;36mfunction_def_to_graph\u001b[1;34m(fdef, structured_input_signature, structured_outputs, input_shapes)\u001b[0m\n\u001b[0;32m     73\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;31m# Add all function nodes to the graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[0mimporter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_graph_def_for_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;31m# Initialize fields specific to FuncGraph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Enes\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\u001b[0m in \u001b[0;36mimport_graph_def_for_function\u001b[1;34m(graph_def, name)\u001b[0m\n\u001b[0;32m    413\u001b[0m   \u001b[1;34m\"\"\"Like import_graph_def but does not validate colocation constraints.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m   return _import_graph_def_internal(\n\u001b[1;32m--> 415\u001b[1;33m       graph_def, validate_colocation_constraints=False, name=name)\n\u001b[0m\u001b[0;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Enes\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\u001b[0m in \u001b[0;36m_import_graph_def_internal\u001b[1;34m(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[1;31m# TODO(skyewm): avoid sending serialized FunctionDefs back to the TF_Graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m     \u001b[0m_ProcessNewOps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlibrary\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Enes\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\u001b[0m in \u001b[0;36m_ProcessNewOps\u001b[1;34m(graph)\u001b[0m\n\u001b[0;32m    245\u001b[0m   \u001b[0mcolocation_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m   \u001b[1;32mfor\u001b[0m \u001b[0mnew_op\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_new_tf_operations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompute_devices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m     \u001b[0moriginal_device\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[0mnew_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Enes\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_add_new_tf_operations\u001b[1;34m(self, compute_devices)\u001b[0m\n\u001b[0;32m   3926\u001b[0m     new_ops = [\n\u001b[0;32m   3927\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_from_tf_operation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_devices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3928\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mc_op\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_tf_operations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3929\u001b[0m     ]\n\u001b[0;32m   3930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Enes\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3926\u001b[0m     new_ops = [\n\u001b[0;32m   3927\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_from_tf_operation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_devices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3928\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mc_op\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_tf_operations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3929\u001b[0m     ]\n\u001b[0;32m   3930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Enes\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_from_tf_operation\u001b[1;34m(self, c_op, compute_device)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \"\"\"\n\u001b[0;32m   3808\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_not_finalized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3809\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3810\u001b[0m     \u001b[1;31m# If a name_scope was created with ret.name but no nodes were created in it,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3811\u001b[0m     \u001b[1;31m# the name will still appear in _names_in_use even though the name hasn't\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Enes\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   2180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2181\u001b[0m     \u001b[1;31m# Initialize self._outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2182\u001b[1;33m     \u001b[0mnum_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_OperationNumOutputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2183\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2184\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "dataset_dir = 'sentiment140'\n",
    "\n",
    "vaccines = [\"biontech\", \"janssen\", \"moderna\",\n",
    "            \"oxford\", \"sinopharm\", \"sinovac\", \"sputnik\"]\n",
    "\n",
    "\n",
    "\n",
    "years = [\"2020\", \"2021\", \"2022\"]\n",
    "months = [\"january\", \"february\", \"march\", \"april\", \"may\", \"june\",\n",
    "          \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"]\n",
    "\n",
    "\n",
    "## MONTHS\n",
    "models_dir = '../models/NN_model_seven_12008898266757515530_0.7899888157844543'\n",
    "tokenizers_dir = '../tokenizers/12008898266757515530.pkl'\n",
    "model = load_model(models_dir)\n",
    "model.summary()\n",
    "tokenizer = joblib.load(tokenizers_dir)\n",
    "word_index = tokenizer.word_index\n",
    "for year in years:\n",
    "    for month in months:\n",
    "    \n",
    "        # already processed\n",
    "        try:\n",
    "            df = pd.read_csv(f'../{year}-data/covid-{month}.csv', delimiter=',')\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        df = df.drop_duplicates()\n",
    "\n",
    "        df = df[['tweet', 'sentiment']]\n",
    "\n",
    "        df.head()\n",
    "        \n",
    "\n",
    "        X = df.iloc[:, 0].fillna(' ')\n",
    "\n",
    "        tweets = X\n",
    "\n",
    "        num_of_tweets_analyzed = len(tweets)\n",
    "\n",
    "        sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "\n",
    "        # Max number of words in a sequence\n",
    "        max_length = 37\n",
    "\n",
    "        padded = pad_sequences(\n",
    "            sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "        # Check reversing the indices\n",
    "\n",
    "        # flip (key, value)\n",
    "        reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])\n",
    "\n",
    "        def decode(sequence):\n",
    "            return \" \".join([reverse_word_index.get(idx, \"?\") for idx in sequence])\n",
    "\n",
    "        predictions = model.predict(padded)\n",
    "\n",
    "        # Only for BinaryCrossentropy\n",
    "        predictions = [1 if p > 0.5 else 0 for p in predictions]\n",
    "\n",
    "        # saving tweets to csv\n",
    "        tweets.to_csv(f'../analysis/tweets-{month}-{year}.csv')\n",
    "        # saving sentiment predictions to csv\n",
    "        np.savetxt(f'../analysis/predictions-{month}-{year}.csv',\n",
    "                   predictions, delimiter=',', fmt=('%s'))\n",
    "\n",
    "        # adding sentiment column to the beginning\n",
    "        df = pd.read_csv(\n",
    "            f'../analysis/predictions-{month}-{year}.csv', header=None)\n",
    "        df.rename(columns={0: 'sentiment'}, inplace=True)\n",
    "        # save to new csv file\n",
    "        df.to_csv(\n",
    "             f'../analysis/predictions-{month}-{year}.csv', index=False)\n",
    "\n",
    "        # merging tweets and predictions\n",
    "        filenames = [f'../analysis/tweets-{month}-{year}.csv',\n",
    "                       f'../analysis/predictions-{month}-{year}.csv']\n",
    "        dfs = []\n",
    "        for filename in filenames:\n",
    "            # read the csv, making sure the first two columns are str\n",
    "            df = pd.read_csv(filename, header=None,\n",
    "                             converters={0: str, 1: str})\n",
    "            # change the column names so they won't collide during concatenation\n",
    "            df.columns = [filename + str(cname) for cname in df.columns]\n",
    "            dfs.append(df)\n",
    "\n",
    "        # concatenate them horizontally\n",
    "        merged = pd.concat(dfs, axis=1)\n",
    "        # write it out\n",
    "        merged.to_csv(\n",
    "            f\"../analysis/merged-{month}-{year}.csv\", header=None, index=None)\n",
    "\n",
    "        df = pd.read_csv(f'../analysis/merged-{month}-{year}.csv')\n",
    "\n",
    "        labels = ['negative', 'positive']\n",
    "\n",
    "        title_type = df.groupby('sentiment').agg('count')\n",
    "\n",
    "        type_labels = ['positive', 'negative']\n",
    "        type_counts = title_type.tweet.sort_values()\n",
    "\n",
    "        colors = ['g', 'r']\n",
    "\n",
    "        plt.subplot(\n",
    "            aspect=1, title=f'Percentage of tweets pro or against vaccination in {month.capitalize()} {year}\\nClassified {num_of_tweets_analyzed} tweets.')\n",
    "        type_show_ids = plt.pie(type_counts, labels=type_labels,\n",
    "                                autopct='%1.1f%%', shadow=True, colors=colors)\n",
    "        plt.savefig(f\"../visuals/{month}-{year}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## WAR!!\n",
    "war = []\n",
    "\n",
    "for file in os.listdir(\"../data/\"):\n",
    "    war.append(file)\n",
    "    \n",
    "\n",
    "models_dir = '../models/NN_model_seven_12008898266757515530_0.7899888157844543'\n",
    "tokenizers_dir = '../tokenizers/12008898266757515530.pkl'\n",
    "model = load_model(models_dir)\n",
    "model.summary()\n",
    "tokenizer = joblib.load(tokenizers_dir)\n",
    "word_index = tokenizer.word_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs0 = []\n",
    "\n",
    "for lang in war:\n",
    "    print(lang)\n",
    "    # already processed\n",
    "    df = pd.read_csv(f'../data/{lang}', delimiter=',')\n",
    "    df = df.drop_duplicates()\n",
    "    df = df[['tweet', 'translate', 'sentiment']]\n",
    "    if not lang == 'war.csv' and not lang == 'war2.csv':\n",
    "        df['tweet'] = df['translate']\n",
    "\n",
    "    X = df.iloc[:, 0].fillna(' ')\n",
    "\n",
    "    tweets = X\n",
    "\n",
    "    num_of_tweets_analyzed = len(tweets)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "    # Max number of words in a sequence\n",
    "    max_length = 37\n",
    "\n",
    "    padded = pad_sequences(\n",
    "        sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "    # Check reversing the indices\n",
    "\n",
    "    # flip (key, value)\n",
    "    reverse_word_index = dict([(idx, word)\n",
    "                                for (word, idx) in word_index.items()])\n",
    "\n",
    "\n",
    "    predictions = model.predict(padded)\n",
    "    \n",
    "    # Only for BinaryCrossentropy\n",
    "    predictions = np.array([1 if p > 0.5 else 0 for p in predictions])\n",
    "    print(np.count_nonzero(predictions == 1), np.count_nonzero(predictions == 0))\n",
    "    \n",
    "    # saving tweets to csv\n",
    "    tweets.to_csv(f'../analysis/tweets-{lang}.csv')\n",
    "    # saving sentiment predictions to csv\n",
    "    np.savetxt(f'../analysis/predictions-{lang}.csv',\n",
    "            predictions, delimiter=',', fmt=('%s'))\n",
    "    # adding sentiment column to the beginning\n",
    "    df = pd.read_csv(\n",
    "        f'../analysis/predictions-{lang}.csv', header=None)\n",
    "    df.rename(columns={0: 'sentiment'}, inplace=True)\n",
    "    # save to new csv file\n",
    "    df.to_csv(\n",
    "        f'../analysis/predictions-{lang}.csv', index=False)\n",
    "    # merging tweets and predictions\n",
    "    filenames = [f'../analysis/tweets-{lang}.csv',\n",
    "                f'../analysis/predictions-{lang}.csv']\n",
    "    dfs = []\n",
    "    for filename in filenames:\n",
    "        # read the csv, making sure the first two columns are str\n",
    "        df = pd.read_csv(filename, header=None,\n",
    "                        converters={0: str, 1: str})\n",
    "        # change the column names so they won't collide during concatenation\n",
    "        df.columns = [filename + str(cname) for cname in df.columns]\n",
    "        dfs.append(df)\n",
    "    # concatenate them horizontally\n",
    "    merged = pd.concat(dfs, axis=1)\n",
    "    # write it out\n",
    "    merged.to_csv(\n",
    "        f\"../analysis/merged-{lang}.csv\", header=None, index=None)\n",
    "    df = pd.read_csv(f'../analysis/merged-{lang}.csv')\n",
    "    title_type = df.groupby('sentiment').agg('count')\n",
    "    type_labels = ['positive', 'negative']\n",
    "    type_counts = title_type.tweet.sort_values()\n",
    "    accs0.append(type_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = np.array(accs0)\n",
    "f2 = lambda x: [round(x[0]/(x[0]+x[1])*100, 2), round(x[1]/(x[0]+x[1])*100, 2)]\n",
    "# accs[-2] = 0\n",
    "accs = [f2(x) for x in accs]\n",
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [x[0]+x[1] for x in accs0]\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = [ \"french\", \"german\", \"italian\", \"norwegian\", \"polish\", \"russian\", \"spanish\", \"english\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(langs, counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycler import cycler\n",
    "\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots()\n",
    "plt.xticks(rotation=90)\n",
    "plt.rc('axes', prop_cycle=(cycler('color', ['g', 'r'])))\n",
    "ax.plot(langs, accs, ls='-', marker='o', label=['Positive', 'Negative'])  \n",
    "\n",
    "plt.xlabel(\"Language\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.title(\"Positive and Negative Sentiment Across Languages with Bi-LSTM\")\n",
    "ax.legend()\n",
    "plt.savefig(f\"../visuals/yayin/rnn/war.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VACCINES\n",
    "models_dir = '../models/NN_model_seven_12008898266757515530_0.7899888157844543'\n",
    "tokenizers_dir = '../tokenizers/12008898266757515530.pkl'\n",
    "model = load_model(models_dir)\n",
    "model.summary()\n",
    "tokenizer = joblib.load(tokenizers_dir)\n",
    "word_index = tokenizer.word_index\n",
    "for vaccine in vaccines:\n",
    "    \n",
    "    \n",
    "    # already processed\n",
    "    df = pd.read_csv(f'../vaccines/{vaccine}.csv', delimiter=',')\n",
    "    df = df.drop_duplicates()\n",
    "    df = df[['tweet', 'sentiment']]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    X = df.iloc[:, 0].fillna(' ')\n",
    "\n",
    "    tweets = X\n",
    "\n",
    "    num_of_tweets_analyzed = len(tweets)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "    # Max number of words in a sequence\n",
    "    max_length = 37\n",
    "\n",
    "    padded = pad_sequences(\n",
    "          sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "    # Check reversing the indices\n",
    "\n",
    "    # flip (key, value)\n",
    "    reverse_word_index = dict([(idx, word)\n",
    "                                  for (word, idx) in word_index.items()])\n",
    "\n",
    "    def decode(sequence):\n",
    "        return \" \".join([reverse_word_index.get(idx, \"?\") for idx in sequence])\n",
    "\n",
    "    predictions = model.predict(padded)\n",
    "\n",
    "    # Only for BinaryCrossentropy\n",
    "    predictions = [1 if p > 0.5 else 0 for p in predictions]\n",
    "    # saving tweets to csv\n",
    "    tweets.to_csv(f'../analysis/tweets-{vaccine}.csv')\n",
    "    # saving sentiment predictions to csv\n",
    "    np.savetxt(f'../analysis/predictions-{vaccine}.csv',\n",
    "               predictions, delimiter=',', fmt=('%s'))\n",
    "    # adding sentiment column to the beginning\n",
    "    df = pd.read_csv(\n",
    "        f'../analysis/predictions-{vaccine}.csv', header=None)\n",
    "    df.rename(columns={0: 'sentiment'}, inplace=True)\n",
    "    # save to new csv file\n",
    "    df.to_csv(\n",
    "         f'../analysis/predictions-{vaccine}.csv', index=False)\n",
    "    # merging tweets and predictions\n",
    "    filenames = [f'../analysis/tweets-{vaccine}.csv',\n",
    "                   f'../analysis/predictions-{vaccine}.csv']\n",
    "    dfs = []\n",
    "    for filename in filenames:\n",
    "        # read the csv, making sure the first two columns are str\n",
    "        df = pd.read_csv(filename, header=None,\n",
    "                         converters={0: str, 1: str})\n",
    "        # change the column names so they won't collide during concatenation\n",
    "        df.columns = [filename + str(cname) for cname in df.columns]\n",
    "        dfs.append(df)\n",
    "    # concatenate them horizontally\n",
    "    merged = pd.concat(dfs, axis=1)\n",
    "    # write it out\n",
    "    merged.to_csv(\n",
    "        f\"../analysis/merged-{vaccine}.csv\", header=None, index=None)\n",
    "    df = pd.read_csv(f'../analysis/merged-{vaccine}.csv')\n",
    "    labels = ['negative', 'positive']\n",
    "    title_type = df.groupby('sentiment').agg('count')\n",
    "    type_labels = ['positive', 'negative']\n",
    "    type_counts = title_type.tweet.sort_values()\n",
    "    colors = ['g', 'r']\n",
    "    plt.subplot(\n",
    "        aspect=1, title=f'Percentage of tweets pro or against vaccines branded by {vaccine}\\nClassified {num_of_tweets_analyzed} tweets.')\n",
    "    type_show_ids = plt.pie(type_counts, labels=type_labels,\n",
    "                            autopct='%1.1f%%', shadow=True, colors=colors)\n",
    "    plt.savefig(f\"../visuals/{vaccine}.png\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f595b24bd0005ade19c9cc9195ebfd43399e9f8b470abdede700a27b5c9ee90b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
