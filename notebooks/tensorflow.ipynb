{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import uuid\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueid = uuid.uuid4().int & (1 << 64)-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pre_trained_embeds = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dir = \"imdb\"\n",
    "dataset_dir = \"sentiment140\"\n",
    "# dataset_dir = \"coronaNLP\"\n",
    "\n",
    "model_dir = \"models\"\n",
    "visuals_dir = \"visuals\"\n",
    "tokenizers_dir = \"tokenizers\"\n",
    "\n",
    "# load a preprocessed dataframe see: (https://github.com/3nws/twitter-text-classification/blob/main/notebooks/process_dataframes.ipynb)\n",
    "df = joblib.load(f\"../dataframes/df_{dataset_dir}.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=[\"text\", \"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[730000:850000] if dataset_dir == \"sentiment140\" else df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(df.sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count unique words\n",
    "def counter_word(text_col):\n",
    "    count = Counter()\n",
    "    for text in text_col.values:\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "counter = counter_word(df.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_words = len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and validation set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = df.iloc[:, 0], df.iloc[:, 1]\n",
    "\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(X, y, test_size=0.3, stratify=y ,random_state=42)\n",
    "\n",
    "train_sentences = train_sentences.to_numpy()\n",
    "val_sentences = val_sentences.to_numpy()\n",
    "train_labels = train_labels.to_numpy()\n",
    "val_labels = val_labels.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences.shape, val_sentences.shape, train_labels.shape, val_labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_sentences), type(val_sentences), type(train_labels), type(val_labels),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences[:1], train_labels[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_features = 50000\n",
    "# max_features = num_unique_words\n",
    "\n",
    "# vectorize a text corpus by turning each text into a sequence of integers\n",
    "tokenizer = Tokenizer(num_words=max_features, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_sentences) # fit only to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each word has unique index\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_of_vocab = len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_sentences[14:15])\n",
    "print(train_sequences[14:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences to have the same length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Max number of words in a sequence\n",
    "max_length = max([len(text) for text in train_sequences]) if dataset_dir == \"sentiment140\" else 50\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "val_padded = pad_sequences(val_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "train_padded.shape, val_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_padded, train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_padded, val_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check reversing the indices\n",
    "\n",
    "# flip (key, value)\n",
    "reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(sequence):\n",
    "    return \" \".join([reverse_word_index.get(idx, \"?\") for idx in sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = decode(train_sequences[10])\n",
    "\n",
    "print(train_sequences[10])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "lstm_dim = int(embedding_dim/2)\n",
    "\n",
    "if use_pre_trained_embeds:\n",
    "    embeddings_dictionary = dict()\n",
    "    glove_file = open('../embeds/glove.6B.300d.txt', 'rb')\n",
    "\n",
    "    for line in glove_file:\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dictionary[word] = vector_dimensions\n",
    "\n",
    "    glove_file.close()\n",
    "\n",
    "    embeddings_matrix = np.zeros((num_unique_words, embedding_dim))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_dictionary.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embeddings_matrix[index] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional, SpatialDropout1D, Dropout, GlobalMaxPool1D, GlobalMaxPool2D\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.regularizers import L1, L2\n",
    "from keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'softmax' activation function returns a probability distribution\n",
    "# Binary for 0-1, Categorical for 2 or more classes, SparseCategorical for when labels are integers\n",
    "# Dropout is used to prevent overfitting by randomly setting inputs to 0 at a low rate\n",
    "# For stacked LSTMs set return_sequences to True except for the last one\n",
    "# trainable parameter in Embedding layer should still be set to True when using already trained weights (it is by default anyway)\n",
    "\n",
    "# 0\n",
    "def one():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_dim,\n",
    "                        input_length=max_length, name=\"embeddinglayer\", weights=[embeddings_matrix], trainable=True))\n",
    "    model.add(Bidirectional(LSTM(lstm_dim, dropout=0.3, return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(lstm_dim, dropout=0.3)))\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "    loss = SparseCategoricalCrossentropy(from_logits=False)\n",
    "    optim = Adam(learning_rate=0.001)\n",
    "    metrics = [\n",
    "        \"accuracy\",\n",
    "        \"sparse_categorical_accuracy\",\n",
    "    ]\n",
    "    model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "# 1\n",
    "def two():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_dim,\n",
    "                               input_length=max_length, name=\"embeddinglayer\"))\n",
    "    model.add(LSTM(embedding_dim, dropout=0.1))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    loss = BinaryCrossentropy(from_logits=False)\n",
    "    optim = Adam(learning_rate=0.001)\n",
    "    metrics = [\n",
    "        \"accuracy\",\n",
    "        \"binary_accuracy\",\n",
    "    ]\n",
    "    model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "    return model\n",
    "    \n",
    "# 2\n",
    "def three():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_dim,\n",
    "                               input_length=max_length, name=\"embeddinglayer\"))\n",
    "    model.add(LSTM(embedding_dim, dropout=0.1))\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "    loss = SparseCategoricalCrossentropy(from_logits=False)\n",
    "    optim = Adam(learning_rate=0.001)\n",
    "    metrics = [\n",
    "        \"accuracy\",\n",
    "        \"sparse_categorical_accuracy\",\n",
    "    ]\n",
    "    model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "# 3\n",
    "def four():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_dim,\n",
    "                               input_length=max_length, name=\"embeddinglayer\"))\n",
    "    model.add(Bidirectional(LSTM(lstm_dim, dropout=0.2)))\n",
    "    model.add(Dense(32, activation=\"relu\"))\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "    loss = SparseCategoricalCrossentropy(from_logits=False)\n",
    "    optim = Adam(learning_rate=0.001)\n",
    "    metrics = [\n",
    "        \"accuracy\",\n",
    "        # \"sparse_categorical_accuracy\",\n",
    "    ]\n",
    "    model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "# 4\n",
    "def five():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_dim,\n",
    "                        input_length=max_length, name=\"embeddinglayer\"))\n",
    "    model.add(Bidirectional(LSTM(lstm_dim, dropout=0.3, return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(lstm_dim, dropout=0.3)))\n",
    "    model.add(Dense(32, activation=\"relu\", kernel_regularizer=L1(0.01), activity_regularizer=L2(0.01)))\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "    loss = SparseCategoricalCrossentropy(from_logits=False)\n",
    "    optim = Adam(learning_rate=0.0001)\n",
    "    metrics = [\n",
    "        \"accuracy\",\n",
    "        \"sparse_categorical_accuracy\",\n",
    "    ]\n",
    "    model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "# 5\n",
    "def six():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Embedding(\n",
    "        max_features, embedding_dim, input_length=max_length))\n",
    "    model.add(SpatialDropout1D(0.4))\n",
    "    model.add(Bidirectional(LSTM(lstm_dim, dropout=0.05, recurrent_dropout=0.2)))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    loss = SparseCategoricalCrossentropy(from_logits=False)\n",
    "    optim = Adam(learning_rate=0.001)\n",
    "    metrics = [\"accuracy\",\n",
    "               \"sparse_categorical_accuracy\",\n",
    "    ]\n",
    "    model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "# 6\n",
    "def seven():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_dim,\n",
    "                        input_length=max_length, name=\"embeddinglayer\"))\n",
    "    model.add(Bidirectional(LSTM(lstm_dim, dropout=0.3, return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(lstm_dim, dropout=0.3)))\n",
    "    model.add(Dense(32, activation=\"relu\", kernel_regularizer=L1(0.01),\n",
    "                    activity_regularizer=L2(0.01)))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    loss = BinaryCrossentropy(from_logits=False)\n",
    "    optim = Adam(learning_rate=1e-4)\n",
    "    metrics = [\n",
    "        \"accuracy\",\n",
    "    ]\n",
    "    model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "# 7\n",
    "def eight():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 128))\n",
    "    model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(20, activation=\"relu\"))\n",
    "    model.add(Dropout(0.05))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    loss = BinaryCrossentropy(from_logits=False)\n",
    "    optim = Adam(learning_rate=1e-4)\n",
    "    metrics = [\n",
    "        \"accuracy\",\n",
    "    ]\n",
    "    model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "# 8\n",
    "def nine():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_dim,\n",
    "                        input_length=max_length, name=\"embeddinglayer\"))\n",
    "    model.add(Bidirectional(LSTM(lstm_dim, dropout=0.3, return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(lstm_dim, dropout=0.3)))\n",
    "    model.add(Dense(32, activation=\"relu\", kernel_regularizer=L1(0.01),\n",
    "                    activity_regularizer=L2(0.01)))\n",
    "    model.add(Dense(5, activation=\"softmax\"))\n",
    "    loss = SparseCategoricalCrossentropy(from_logits=False)\n",
    "    optim = Adam(learning_rate=1e-4)\n",
    "    metrics = [\n",
    "        \"accuracy\",\n",
    "    ]\n",
    "    model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features, embedding_dim, max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    one,\n",
    "    two,\n",
    "    three,\n",
    "    four,\n",
    "    five,\n",
    "    six,\n",
    "    seven,\n",
    "    eight,\n",
    "    nine\n",
    "]\n",
    "\n",
    "model_to_use = -3\n",
    "\n",
    "model_idx = 0 if use_pre_trained_embeds else model_to_use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[model_idx]()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=3, batch_size=128, validation_data=val_dataset, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging purposes\n",
    "\n",
    "outputs = [layer.output for layer in model.layers]\n",
    "\n",
    "debug_model = Model(inputs=model.input,\n",
    "                    outputs=outputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = debug_model.predict(val_padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for feature, output in zip(features, outputs):\n",
    "    print(output.name)\n",
    "    print(feature.shape)\n",
    "    print(feature[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_loss, val_acc = model.evaluate(val_dataset)\n",
    "val_loss, val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = models[model_idx].__name__\n",
    "model_export = f\"NN_model_{model_name}_{uniqueid}_{val_acc}\"\n",
    "vis_dir = f'../{visuals_dir}/{model_export}'\n",
    "model_save_dir = f'../{model_dir}/{model_export}'\n",
    "tokenizer_dir = f'../{tokenizers_dir}/{uniqueid}'\n",
    "\n",
    "# plotting training graph\n",
    "plt.plot(history.history['loss'])\n",
    "plt.savefig(f'{vis_dir}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_sentences[2])\n",
    "print(val_labels[2])\n",
    "print(model.predict(val_padded[2:3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = model.predict(val_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for BinaryCrossentropy\n",
    "predictions = [1 if p > 0.5 else 0 for p in val_predictions]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.ylim(0, None)\n",
    "plt.savefig(f'{vis_dir}_loss_acc.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(model_save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(tokenizer, f'{tokenizer_dir}.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_tokenizer = joblib.load(f'{tokenizer_dir}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_tokenizer\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "599e51aa143538ccec1c7ab4b528efe64565e20e387dbc49eb00bf436cfb223b"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
