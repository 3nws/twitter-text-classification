{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import uuid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueid = uuid.uuid4().int & (1 << 64)-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pre_trained_embeds = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"imdb\"\n",
    "model_dir = \"models\"\n",
    "\n",
    "# load a preprocessed dataframe see: (https://github.com/3nws/twitter-text-classification/blob/main/notebooks/process_dataframes.ipynb)\n",
    "df = joblib.load(\"../dataframes/df_imdb.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(df.sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count unique words\n",
    "def counter_word(text_col):\n",
    "    count = Counter()\n",
    "    for text in text_col.values:\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "counter = counter_word(df.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_words = len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and validation set\n",
    "train_size = int(df.shape[0] * 0.8)\n",
    "\n",
    "train_df = df[:train_size]\n",
    "val_df = df[train_size:]\n",
    "\n",
    "# split text and labels\n",
    "train_sentences = train_df.review.to_numpy()\n",
    "train_labels = train_df.sentiment.to_numpy()\n",
    "val_sentences = val_df.review.to_numpy()\n",
    "val_labels = val_df.sentiment.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences.shape, val_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# vectorize a text corpus by turning each text into a sequence of integers\n",
    "tokenizer = Tokenizer(num_words=num_unique_words)\n",
    "tokenizer.fit_on_texts(train_sentences) # fit only to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each word has unique index\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_of_vocab = len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_sentences[10:15])\n",
    "print(train_sequences[10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences to have the same length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Max number of words in a sequence\n",
    "# max_length = max([len(text) for text in train_sequences])\n",
    "max_length = 128\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "val_padded = pad_sequences(val_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "train_padded.shape, val_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_sentences[10])\n",
    "print(train_sequences[10])\n",
    "print(train_padded[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check reversing the indices\n",
    "\n",
    "# flip (key, value)\n",
    "reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(sequence):\n",
    "    return \" \".join([reverse_word_index.get(idx, \"?\") for idx in sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = decode(train_sequences[10])\n",
    "\n",
    "print(train_sequences[10])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pre_trained_embeds:\n",
    "    embeddings_dictionary = dict()\n",
    "    embedding_dim = 32\n",
    "    glove_file = open('../embeds/glove.6B.300d.txt', 'rb')\n",
    "\n",
    "    for line in glove_file:\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dictionary[word] = vector_dimensions\n",
    "\n",
    "    glove_file.close()\n",
    "\n",
    "    embeddings_matrix = np.zeros((num_unique_words, embedding_dim))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_dictionary.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embeddings_matrix[index] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'softmax' activation function returns a probability distribution\n",
    "# Binary for 0-1, Categorical for 2 or more classes, SparseCategorical for when labels are integers\n",
    "# Dropout is used to prevent overfitting by randomly setting inputs to 0 at a low rate\n",
    "# For stacked LSTMs set return_sequences to True except for the last one\n",
    "# trainable parameter in Embedding layer should still be set to True when using already trained weights (it is by default anyway)\n",
    "\n",
    "# 0\n",
    "def Glove_Double_Bi_LSTM_w_Loss_Sparse_Cat():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_unique_words, embedding_dim,\n",
    "                        input_length=max_length, name=\"embeddinglayer\", weights=[embeddings_matrix], trainable=True))\n",
    "    model.add(Bidirectional(LSTM(64, dropout=0.3, return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(64, dropout=0.3)))\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "    loss = SparseCategoricalCrossentropy(from_logits=False)\n",
    "    optim = Adam(lr=0.001)\n",
    "    metrics = [\n",
    "        \"accuracy\",\n",
    "        \"sparse_categorical_accuracy\",\n",
    "    ]\n",
    "    model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "# 1\n",
    "def LSTM_w_Loss_Binary():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_unique_words, embedding_dim,\n",
    "                               input_length=max_length, name=\"embeddinglayer\"))\n",
    "    model.add(LSTM(64, dropout=0.1))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    loss = BinaryCrossentropy(from_logits=False)\n",
    "    optim = Adam(lr=0.001)\n",
    "    metrics = [\n",
    "        \"accuracy\",\n",
    "        \"binary_accuracy\",\n",
    "    ]\n",
    "    model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "    return model\n",
    "    \n",
    "# 2\n",
    "def LSTM_w_Loss_Sparse_Cat():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_unique_words, embedding_dim,\n",
    "                               input_length=max_length, name=\"embeddinglayer\"))\n",
    "    model.add(LSTM(64, dropout=0.1))\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "    loss = SparseCategoricalCrossentropy(from_logits=False)\n",
    "    optim = Adam(lr=0.001)\n",
    "    metrics = [\n",
    "        \"accuracy\",\n",
    "        \"sparse_categorical_accuracy\",\n",
    "    ]\n",
    "    model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "# 3\n",
    "def Bi_LSTM_w_Loss_Sparse_Cat():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_unique_words, embedding_dim,\n",
    "                               input_length=max_length, name=\"embeddinglayer\"))\n",
    "    model.add(Bidirectional(LSTM(64, dropout=0.1)))\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "    loss = SparseCategoricalCrossentropy(from_logits=False)\n",
    "    optim = Adam(lr=0.001)\n",
    "    metrics = [\n",
    "        \"accuracy\",\n",
    "        \"sparse_categorical_accuracy\",\n",
    "    ]\n",
    "    model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "# 4\n",
    "def Double_Bi_LSTM_w_Loss_Sparse_Cat():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_unique_words, embedding_dim,\n",
    "                        input_length=max_length, name=\"embeddinglayer\"))\n",
    "    model.add(Bidirectional(LSTM(64, dropout=0.3, return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(64, dropout=0.3)))\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "    loss = SparseCategoricalCrossentropy(from_logits=False)\n",
    "    optim = Adam(lr=0.001)\n",
    "    metrics = [\n",
    "        \"accuracy\",\n",
    "        \"sparse_categorical_accuracy\",\n",
    "    ]\n",
    "    model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    Glove_Double_Bi_LSTM_w_Loss_Sparse_Cat,\n",
    "    LSTM_w_Loss_Binary,\n",
    "    LSTM_w_Loss_Sparse_Cat,\n",
    "    Bi_LSTM_w_Loss_Sparse_Cat,\n",
    "    Double_Bi_LSTM_w_Loss_Sparse_Cat,\n",
    "]\n",
    "\n",
    "model_to_use = -1\n",
    "\n",
    "model_idx = 0 if use_pre_trained_embeds else model_to_use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = models[model_idx]()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_padded, train_labels, epochs=4, validation_data=(val_padded, val_labels), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plotting training graph\n",
    "\n",
    "plt.plot(history.history['loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(train_padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = model.predict(val_padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_sentences[42])\n",
    "print(val_labels[42])\n",
    "print(val_predictions[42])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for BinaryCrossentropy\n",
    "# predictions = [1 if p > 0.5 else 0 for p in predictions]\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_loss, val_acc = model.evaluate(val_padded, val_labels)\n",
    "val_loss, val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.ylim(0, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = f'../{model_dir}/NN_model_{model_idx}_{uniqueid}_{val_acc}'\n",
    "\n",
    "model.save(model_save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(model_save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging purposes\n",
    "\n",
    "\n",
    "# model = keras.Model(inputs=model.input,\n",
    "#                     outputs=[model.get_layer(\"embeddingL\").output])\n",
    "\n",
    "# feature = model.predict(val_padded)\n",
    "\n",
    "# feature, feature.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "599e51aa143538ccec1c7ab4b528efe64565e20e387dbc49eb00bf436cfb223b"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
