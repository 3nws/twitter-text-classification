{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# importing the dataset\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "# DATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"tweet\"]\n",
    "# df = pd.read_csv('./training.1600000.processed.noemoticon.csv', delimiter=',', encoding=DATASET_ENCODING , names=DATASET_COLUMNS)\n",
    "\n",
    "df = pd.read_csv('./IMDB Dataset.csv', delimiter=',',\n",
    "                 encoding=DATASET_ENCODING)\n",
    "# df = pd.read_csv('./Corona_NLP_train.csv',\n",
    "# delimiter=',', encoding=DATASET_ENCODING)\n",
    "dataset_dir = 'imdb'\n",
    "# dataset_dir = 'coronaNLP'\n",
    "# dataset_dir = 'sentiment140'\n",
    "model_dir = './models/'+dataset_dir\n",
    "vector_dir = './vectors/'+dataset_dir\n",
    "\n",
    "# removing the unnecessary columns and duplicates\n",
    "# dataset = dataset[['OriginalTweet','Sentiment']]\n",
    "# df = df[['tweet', 'sentiment']]\n",
    "df = df[['review', 'sentiment']]\n",
    "df.drop_duplicates()\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r\"\", text)\n",
    "\n",
    "\n",
    "def remove_punct(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "def remove_mention(text):\n",
    "    return re.sub(\"@[A-Za-z0-9]+\", \"\", text)\n",
    "\n",
    "\n",
    "def stem_tweets(tweet):\n",
    "    tokens = tweet.split()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "\n",
    "# remove stopwords\n",
    "\n",
    "\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "    filtered_words = [word.lower()\n",
    "                      for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "def preprocess_tweets(tweet):\n",
    "    tweet = remove_mention(tweet)\n",
    "    tweet = remove_URL(tweet)\n",
    "    tweet = remove_punct(tweet)\n",
    "    tweet = stem_tweets(tweet)\n",
    "    tweet = remove_stopwords(tweet)\n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.head(5)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df['tweet'] = df.tweet.apply(preprocess_tweets)\n",
    "df['review'] = df.review.apply(preprocess_tweets)\n",
    "# df['OriginalTweet'] = df.OriginalTweet.apply(preprocess_tweets)\n",
    "\n",
    "X = df['review']\n",
    "# X = df['tweet']\n",
    "# X = df['OriginalTweet']\n",
    "y = df['sentiment']\n",
    "# y = df['Sentiment']\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count unique words\n",
    "\n",
    "\n",
    "def counter_word(text_col):\n",
    "    count = Counter()\n",
    "    for text in text_col.values:\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "# counter = counter_word(df.tweet)\n",
    "# counter = counter_word(df.OriginalTweet)\n",
    "counter = counter_word(df.review)\n",
    "\n",
    "len(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_words = len(counter)\n",
    "counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.most_common(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and validation set\n",
    "train_size = int(df.shape[0] * 0.8)\n",
    "\n",
    "train_df = df[:train_size]\n",
    "val_df = df[train_size:]\n",
    "\n",
    "# split text and labels\n",
    "# train_sentences = train_df.tweet.to_numpy()\n",
    "# train_labels = train_df.sentiment.to_numpy()\n",
    "# val_sentences = val_df.tweet.to_numpy()\n",
    "# val_labels = val_df.sentiment.to_numpy()\n",
    "\n",
    "# train_sentences = train_df.OriginalTweet.to_numpy()\n",
    "# train_labels = train_df.Sentiment.to_numpy()\n",
    "# val_sentences = val_df.OriginalTweet.to_numpy()\n",
    "# val_labels = val_df.Sentiment.to_numpy()\n",
    "\n",
    "train_sentences = train_df.review.to_numpy()\n",
    "train_labels = train_df.sentiment.to_numpy()\n",
    "val_sentences = val_df.review.to_numpy()\n",
    "val_labels = val_df.sentiment.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentiment_to_int(sentiment):\n",
    "    return 1 if sentiment == 'positive' else 0\n",
    "\n",
    "\n",
    "convert_sentiment_to_int_v = np.vectorize(convert_sentiment_to_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = convert_sentiment_to_int_v(train_labels)\n",
    "\n",
    "val_labels = convert_sentiment_to_int_v(val_labels)\n",
    "\n",
    "train_labels, val_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences.shape, val_sentences.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# vectorize a text corpus by turning each text into a sequence of integers\n",
    "tokenizer = Tokenizer(num_words=num_unique_words)\n",
    "tokenizer.fit_on_texts(train_sentences)  # fit only to training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each word has unique index\n",
    "word_index = tokenizer.word_index\n",
    "word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(train_sentences[0])\n",
    "print(train_sequences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences to have the same length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Max number of words in a sequence\n",
    "max_length = 175\n",
    "\n",
    "train_padded = pad_sequences(\n",
    "    train_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "val_padded = pad_sequences(\n",
    "    val_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "train_padded.shape, val_padded.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_sentences[3])\n",
    "print(train_sequences[3])\n",
    "print(train_padded[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check reversing the indices\n",
    "\n",
    "# flip (key, value)\n",
    "reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])\n",
    "reverse_word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(sequence):\n",
    "    return \" \".join([reverse_word_index.get(idx, \"?\") for idx in sequence])\n",
    "\n",
    "\n",
    "decoded_text = decode(train_sequences[3])\n",
    "\n",
    "print(train_sequences[3])\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM model\n",
    "from tensorflow.keras import layers\n",
    "import keras\n",
    "\n",
    "# Embedding: https://www.tensorflow.org/tutorials/text/word_embeddings\n",
    "# Turns positive integers (indexes) into dense vectors of fixed size. (other approach could be one-hot-encoding)\n",
    "\n",
    "# Word embeddings give us a way to use an efficient, dense representation in which similar words have\n",
    "# a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a\n",
    "# dense vector of floating point values (the length of the vector is a parameter you specify).\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(num_unique_words, 32, input_length=max_length))\n",
    "\n",
    "# The layer will take as input an integer matrix of size (batch, input_length),\n",
    "# and the largest integer (i.e. word index) in the input should be no larger than num_words (vocabulary size).\n",
    "# Now model.output_shape is (None, input_length, 32), where `None` is the batch dimension.\n",
    "\n",
    "\n",
    "model.add(layers.LSTM(64, dropout=0.1))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "optim = keras.optimizers.Adam(lr=0.001)\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "model.compile(loss=loss, optimizer=optim, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_padded, train_labels, epochs=3,\n",
    "          validation_data=(val_padded, val_labels), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(train_padded)\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = [1 if p > 0.5 else 0 for p in predictions]\n",
    "print(train_sentences[:3])\n",
    "\n",
    "print(train_labels[:3])\n",
    "print(predictions[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_loss, val_acc = model.evaluate(val_padded, val_labels)\n",
    "val_loss, val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'{model_dir}/MNB_model_{val_acc}')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "599e51aa143538ccec1c7ab4b528efe64565e20e387dbc49eb00bf436cfb223b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
