{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# importing the dataset\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "# DATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"tweet\"]\n",
    "# dataset = pd.read_csv('./training.1600000.processed.noemoticon.csv', delimiter=',', encoding=DATASET_ENCODING , names=DATASET_COLUMNS)\n",
    "\n",
    "dataset = pd.read_csv('./IMDB Dataset.csv', delimiter=',',\n",
    "                      encoding=DATASET_ENCODING)\n",
    "# dataset = pd.read_csv('./Corona_NLP_train.csv',\n",
    "#   delimiter=',', encoding=DATASET_ENCODING)\n",
    "dataset_dir = 'imdb'\n",
    "# dataset_dir = 'coronaNLP'\n",
    "# dataset_dir = 'sentiment140'\n",
    "model_dir = './models/'+dataset_dir\n",
    "vector_dir = './vectors/'+dataset_dir\n",
    "\n",
    "# removing the unnecessary columns and duplicates\n",
    "# dataset = dataset[['OriginalTweet','Sentiment']]\n",
    "# dataset = dataset[['tweet','sentiment']]\n",
    "dataset = dataset[['review', 'sentiment']]\n",
    "dataset.drop_duplicates()\n",
    "\n",
    "# dataset.head()\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r\"\", text)\n",
    "\n",
    "\n",
    "def remove_punct(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "def remove_mention(text):\n",
    "    return re.sub(\"@[A-Za-z0-9]+\", \"\", text)\n",
    "\n",
    "\n",
    "def stem_tweets(tweet):\n",
    "    tokens = tweet.split()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "\n",
    "# remove stopwords\n",
    "\n",
    "\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "    filtered_words = [word.lower()\n",
    "                      for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "def preprocess_tweets(tweet):\n",
    "    tweet = remove_mention(tweet)\n",
    "    tweet = remove_URL(tweet)\n",
    "    tweet = remove_punct(tweet)\n",
    "    tweet = stem_tweets(tweet)\n",
    "    tweet = remove_stopwords(tweet)\n",
    "    return tweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = dataset['review']\n",
    "# X = dataset['tweet']\n",
    "# X = dataset['OriginalTweet']\n",
    "\n",
    "X = X.apply(preprocess_tweets)\n",
    "\n",
    "y = dataset['sentiment']\n",
    "# y = dataset['Sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf = pickle.load(open(f\"./vectors/{dataset_dir}/tfidf_mnb_87\", \"rb\"))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = tfidf.transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creating our pipeline that will return an estimator\n",
    "pipeline = Pipeline([('clf', LogisticRegression(random_state=42, verbose=1))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = {\n",
    "    'clf__C': (1, 0.5),\n",
    "    'clf__penalty': ('l1', 'l2', 'elasticnet', 'none'),\n",
    "    'clf__solver': ('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga')\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(pipeline, param_grid=parameters, cv=5, verbose=3, n_jobs=1)\n",
    "\n",
    "clf = clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"Best: %f using %s\" % (clf.best_score_,\n",
    "                             clf.best_params_))\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "params = clf.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc = int(accuracy_score(y_test, y_pred)*100)\n",
    "\n",
    "# exporting the pipeline\n",
    "# pickle.dump(clf, open(f'{model_dir}/LRG_model_{acc}', 'wb'))\n",
    "# pickle.dump(pipeline['tfidf'], open(f'{vector_dir}/tfidf_lrg_{acc}', 'wb'))\n",
    "\n",
    "joblib.dump(clf.best_estimator_, f'{model_dir}/LRG_model_{acc}.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "599e51aa143538ccec1c7ab4b528efe64565e20e387dbc49eb00bf436cfb223b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
